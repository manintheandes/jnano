<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>How jllm Learns — from raw text to neural network</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;600&display=swap" rel="stylesheet">
<style>
  :root {
    --bg: #000; --surface: #0a0a0a; --card: #111; --border: #1f1f1f;
    --text: #fff; --text-2: #999; --text-3: #555;
    --accent: #a855f7; --accent2: #ec4899; --green: #22c55e; --blue: #3b82f6;
    --mono: 'JetBrains Mono', monospace; --sans: 'Inter', sans-serif;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { background: var(--bg); color: var(--text-2); font-family: var(--sans); -webkit-font-smoothing: antialiased; overflow-x: hidden; }
  .container { max-width: 920px; margin: 0 auto; padding: 60px 32px 140px; }
  h1 { font-size: 48px; font-weight: 800; color: var(--text); letter-spacing: -1.5px; line-height: 1.1; margin-bottom: 12px; }
  .subtitle { font-size: 18px; color: var(--text-3); font-weight: 400; margin-bottom: 64px; }

  .scene { display: none; opacity: 0; transition: opacity 0.6s ease; }
  .scene.active { display: block; }
  .scene.visible { opacity: 1; }
  .scene-label { font-size: 12px; text-transform: uppercase; letter-spacing: 3px; font-weight: 600; margin-bottom: 24px; background: linear-gradient(135deg, var(--accent), var(--accent2)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
  .scene-heading { font-size: 32px; font-weight: 700; color: var(--text); margin-bottom: 16px; letter-spacing: -0.5px; }
  .scene-desc { font-size: 16px; line-height: 1.8; color: var(--text-2); margin-bottom: 32px; max-width: 720px; }
  .scene-desc strong { color: var(--text); font-weight: 600; }

  .card { background: var(--card); border: 1px solid var(--border); border-radius: 16px; padding: 28px; margin-bottom: 20px; }
  .card-glow { border: 1px solid transparent; background: linear-gradient(var(--card), var(--card)) padding-box, linear-gradient(135deg, rgba(168,85,247,0.25), rgba(236,72,153,0.25)) border-box; }
  .text-display { font-family: var(--mono); font-size: 17px; line-height: 2.2; min-height: 60px; }

  .char-box { display: inline-block; padding: 4px 2px; border-radius: 4px; transition: all 0.3s; position: relative; cursor: default; }
  .char-box.highlight { background: rgba(168,85,247,0.15); color: var(--accent); }
  .char-box .byte-val { position: absolute; top: -24px; left: 50%; transform: translateX(-50%); font-size: 10px; color: var(--accent); white-space: nowrap; opacity: 0; transition: opacity 0.3s; background: #1a1a1a; padding: 2px 8px; border-radius: 6px; border: 1px solid var(--border); }
  .char-box.show-bytes .byte-val { opacity: 1; }

  .token-container { display: flex; flex-wrap: wrap; gap: 10px; }
  .token-box { display: inline-flex; flex-direction: column; align-items: center; padding: 14px 18px; border-radius: 12px; font-family: var(--mono); opacity: 0; transform: translateY(10px); transition: all 0.5s ease; }
  .token-box.visible { opacity: 1; transform: translateY(0); }
  .token-text { font-size: 15px; color: var(--text); font-weight: 600; }
  .token-id { font-size: 11px; color: var(--text-3); margin-top: 6px; }

  .merge-step { background: var(--card); border: 1px solid var(--border); border-radius: 14px; padding: 22px 28px; margin-bottom: 14px; opacity: 0; transform: translateX(-16px); transition: all 0.5s ease; }
  .merge-step.visible { opacity: 1; transform: translateX(0); }
  .step-num { font-size: 11px; color: var(--accent); text-transform: uppercase; letter-spacing: 2px; font-weight: 600; margin-bottom: 10px; }
  .merge-pair { font-family: var(--mono); font-size: 17px; color: var(--text); margin-bottom: 6px; display: flex; align-items: center; flex-wrap: wrap; gap: 4px; }
  .merge-old { color: var(--accent); text-decoration: line-through; opacity: 0.5; }
  .merge-arrow { color: var(--text-3); margin: 0 6px; }
  .merge-new { color: var(--green); font-weight: 700; }
  .merge-info { color: var(--text-3); font-size: 13px; margin-left: 12px; }
  .merge-count { font-size: 13px; color: var(--text-3); margin-bottom: 12px; }
  .merge-result { font-family: var(--mono); font-size: 14px; color: var(--blue); padding: 14px; background: var(--bg); border-radius: 10px; line-height: 2; letter-spacing: 1px; }
  .merged { background: rgba(34,197,94,0.1); color: var(--green); padding: 2px 5px; border-radius: 4px; border: 1px solid rgba(34,197,94,0.2); }

  .stats { display: flex; gap: 16px; margin-bottom: 20px; }
  .stat-box { flex: 1; text-align: center; padding: 24px; }
  .stat-value { font-size: 48px; font-weight: 800; letter-spacing: -2px; }
  .stat-label { font-size: 11px; color: var(--text-3); text-transform: uppercase; letter-spacing: 2px; margin-top: 8px; }
  .stat-detail { font-size: 13px; color: var(--text-3); margin-top: 4px; }
  .stat-purple .stat-value { background: linear-gradient(135deg, var(--accent), var(--accent2)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
  .stat-green .stat-value { color: var(--green); }
  .stat-blue .stat-value { color: var(--blue); }

  .insight { background: linear-gradient(135deg, rgba(168,85,247,0.05), rgba(236,72,153,0.03)); border: 1px solid rgba(168,85,247,0.15); border-radius: 14px; padding: 24px 28px; margin-top: 24px; }
  .insight-label { font-size: 11px; text-transform: uppercase; letter-spacing: 2px; font-weight: 600; margin-bottom: 10px; background: linear-gradient(135deg, var(--accent), var(--accent2)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
  .insight p { font-size: 15px; line-height: 1.8; color: var(--text-2); }
  .insight p strong { color: var(--text); }

  .tc0 { background: rgba(168,85,247,0.13); border: 1px solid rgba(168,85,247,0.3); }
  .tc1 { background: rgba(236,72,153,0.13); border: 1px solid rgba(236,72,153,0.3); }
  .tc2 { background: rgba(59,130,246,0.13); border: 1px solid rgba(59,130,246,0.3); }
  .tc3 { background: rgba(245,158,11,0.13); border: 1px solid rgba(245,158,11,0.3); }
  .tc4 { background: rgba(34,197,94,0.13); border: 1px solid rgba(34,197,94,0.3); }
  .tc5 { background: rgba(6,182,212,0.13); border: 1px solid rgba(6,182,212,0.3); }
  .tc6 { background: rgba(244,63,94,0.13); border: 1px solid rgba(244,63,94,0.3); }
  .tc7 { background: rgba(139,92,246,0.13); border: 1px solid rgba(139,92,246,0.3); }

  .ids-display { font-family: var(--mono); font-size: 24px; text-align: center; letter-spacing: 3px; }
  .id-span { transition: opacity 0.5s; }
  .window-row { display: flex; align-items: center; margin-bottom: 18px; }
  .window-label { font-size: 13px; width: 110px; flex-shrink: 0; }
  .bar-outer { flex: 1; background: var(--card); border-radius: 8px; height: 36px; overflow: hidden; position: relative; }
  .bar-inner { height: 100%; border-radius: 8px; transition: width 1.5s ease; }
  .bar-text { position: absolute; top: 50%; left: 14px; transform: translateY(-50%); font-size: 12px; color: #fff; font-weight: 600; }

  /* Embeddings-specific */
  .lookup-row { display: flex; align-items: center; gap: 14px; padding: 16px 0; border-bottom: 1px solid var(--border); font-family: var(--mono); }
  .lookup-row:last-child { border-bottom: none; }
  .lookup-word { color: var(--accent); font-weight: 600; font-size: 17px; min-width: 80px; }
  .lookup-arrow { color: var(--text-3); font-size: 14px; }
  .lookup-id { color: var(--text-2); font-size: 14px; min-width: 90px; }
  .lookup-vec { color: var(--green); font-size: 12px; word-break: break-all; }

  .heatmap { overflow-x: auto; }
  .heat-row { display: flex; align-items: center; gap: 4px; margin-bottom: 4px; }
  .heat-label { font-family: var(--mono); font-size: 14px; color: var(--text); width: 72px; text-align: right; padding-right: 12px; flex-shrink: 0; font-weight: 600; }
  .heat-dim-label { font-family: var(--mono); font-size: 10px; color: var(--text-3); text-transform: uppercase; letter-spacing: 1px; }
  .heat-cell { width: 72px; height: 44px; border-radius: 8px; display: flex; align-items: center; justify-content: center; font-size: 11px; font-family: var(--mono); color: rgba(255,255,255,0.7); flex-shrink: 0; transition: transform 0.2s; cursor: default; }
  .heat-cell:hover { transform: scale(1.08); z-index: 1; }
  .heat-spacer { width: 72px; flex-shrink: 0; }
  .heat-bracket { font-size: 12px; color: var(--text-3); margin-top: 12px; text-align: center; font-family: var(--mono); }

  .sim-row { display: flex; align-items: center; margin-bottom: 10px; }
  .sim-label { font-family: var(--mono); font-size: 13px; color: var(--text-2); width: 140px; flex-shrink: 0; text-align: right; padding-right: 16px; }
  .sim-bar-wrap { flex: 1; background: var(--surface); border-radius: 6px; height: 30px; overflow: hidden; }
  .sim-bar { height: 100%; border-radius: 6px; transition: width 1s ease; min-width: 2px; }
  .sim-val { font-family: var(--mono); font-size: 13px; width: 64px; text-align: right; padding-left: 12px; flex-shrink: 0; }
  .sim-pos { color: var(--accent); }
  .sim-neg { color: var(--blue); }
  .sim-highlight .sim-label { color: var(--text); font-weight: 600; }

  .pca-wrap { border-radius: 14px; overflow: hidden; }
  .pca-canvas { display: block; width: 100%; border-radius: 14px; }

  .pipeline { display: flex; align-items: stretch; gap: 0; flex-wrap: wrap; justify-content: center; margin-top: 8px; }
  .pipe-step { border-radius: 12px; padding: 18px 16px; text-align: center; min-width: 90px; flex: 1; }
  .pipe-done { background: var(--card); border: 1px solid var(--border); }
  .pipe-current { border: 1px solid transparent; background: linear-gradient(var(--card), var(--card)) padding-box, linear-gradient(135deg, rgba(168,85,247,0.4), rgba(236,72,153,0.4)) border-box; }
  .pipe-next { background: var(--surface); border: 1px dashed #2a2a2a; opacity: 0.5; }
  .pipe-name { font-size: 13px; font-weight: 600; color: var(--text); margin-bottom: 4px; }
  .pipe-detail { font-size: 10px; color: var(--text-3); line-height: 1.4; }
  .pipe-status { font-size: 9px; text-transform: uppercase; letter-spacing: 1px; margin-top: 8px; font-weight: 600; }
  .pipe-status-done { color: var(--green); }
  .pipe-status-now { color: var(--accent); }
  .pipe-status-next { color: var(--text-3); }
  .pipe-arrow { color: var(--text-3); font-size: 18px; display: flex; align-items: center; padding: 0 6px; flex-shrink: 0; }

  /* Chapter divider */
  .chapter-break { text-align: center; padding: 20px 0 40px; }
  .chapter-num { font-size: 13px; text-transform: uppercase; letter-spacing: 4px; color: var(--text-3); margin-bottom: 8px; }
  .chapter-title { font-size: 28px; font-weight: 700; color: var(--text); letter-spacing: -0.5px; }

  /* Controls */
  .controls { position: fixed; bottom: 0; left: 0; right: 0; background: rgba(0,0,0,0.92); border-top: 1px solid var(--border); padding: 20px 32px; display: flex; align-items: center; justify-content: center; gap: 20px; z-index: 100; backdrop-filter: blur(24px); }
  .btn { padding: 12px 32px; border-radius: 10px; border: none; font-size: 15px; font-weight: 600; cursor: pointer; transition: all 0.2s; font-family: var(--sans); }
  .btn-primary { background: linear-gradient(135deg, var(--accent), var(--accent2)); color: #fff; }
  .btn-primary:hover { opacity: 0.9; transform: translateY(-1px); }
  .btn-secondary { background: var(--card); color: var(--text-2); border: 1px solid var(--border); }
  .btn-secondary:hover { background: #1a1a1a; color: var(--text); }
  .btn:disabled { opacity: 0.2; cursor: not-allowed; transform: none; }
  .step-indicator { font-size: 13px; color: var(--text-3); min-width: 140px; text-align: center; }
  .progress-bar { position: fixed; top: 0; left: 0; height: 3px; background: linear-gradient(90deg, var(--accent), var(--accent2)); transition: width 0.4s ease; z-index: 200; }
  .spacer { height: 120px; }
  @keyframes blink { 0%,100%{opacity:1} 50%{opacity:0} }
  .cursor { animation: blink 1s infinite; color: var(--accent); }

  /* ── Chapter 3: Attention styles ── */
  .attn-arrow-container { position: relative; padding: 40px 20px 20px; min-height: 280px; }
  .attn-token-row { display: flex; justify-content: center; gap: 6px; flex-wrap: wrap; margin-bottom: 20px; }
  .attn-token { font-family: var(--mono); font-size: 13px; padding: 8px 10px; border-radius: 8px; background: var(--card); border: 1px solid var(--border); color: var(--text-2); transition: all 0.3s; cursor: default; text-align: center; min-width: 40px; }
  .attn-token.source { background: linear-gradient(135deg, rgba(168,85,247,0.2), rgba(236,72,153,0.2)); border-color: var(--accent); color: var(--text); font-weight: 600; }
  .attn-token.target { border-color: var(--green); }
  .attn-weight-label { font-size: 10px; color: var(--accent); display: block; margin-top: 4px; font-weight: 600; }

  .attn-grid { display: grid; gap: 2px; margin: 0 auto; }
  .attn-cell { width: 100%; aspect-ratio: 1; border-radius: 3px; transition: all 0.3s; cursor: default; position: relative; }
  .attn-cell:hover { transform: scale(1.3); z-index: 10; }
  .attn-axis-label { font-family: var(--mono); font-size: 10px; color: var(--text-3); text-align: center; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; display: flex; align-items: center; justify-content: center; }
  .attn-axis-label.row-label { justify-content: flex-end; padding-right: 6px; }
  .attn-grid-title { font-size: 14px; font-weight: 600; color: var(--text); text-align: center; margin-bottom: 12px; }
  .attn-grid-subtitle { font-size: 12px; color: var(--text-3); text-align: center; margin-bottom: 16px; }
  .attn-tooltip { position: fixed; background: #1a1a1a; border: 1px solid var(--border); border-radius: 8px; padding: 8px 12px; font-family: var(--mono); font-size: 11px; color: var(--text); pointer-events: none; z-index: 1000; white-space: nowrap; }

  .head-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(130px, 1fr)); gap: 12px; }
  .head-card { background: var(--card); border: 1px solid var(--border); border-radius: 12px; padding: 14px; transition: all 0.3s; opacity: 0; transform: translateY(10px); }
  .head-card.visible { opacity: 1; transform: translateY(0); }
  .head-card.highlight { border-color: var(--accent); }
  .head-num { font-size: 10px; text-transform: uppercase; letter-spacing: 2px; color: var(--text-3); margin-bottom: 8px; font-weight: 600; }
  .head-role { font-size: 11px; color: var(--accent); font-weight: 600; margin-bottom: 10px; }
  .head-bar-row { display: flex; align-items: center; gap: 4px; margin-bottom: 3px; }
  .head-bar-label { font-family: var(--mono); font-size: 9px; color: var(--text-3); width: 52px; text-align: right; flex-shrink: 0; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; }
  .head-bar { height: 6px; border-radius: 3px; transition: width 0.8s ease; }

  .evo-layer { margin-bottom: 20px; opacity: 0; transform: translateX(-16px); transition: all 0.5s ease; }
  .evo-layer.visible { opacity: 1; transform: translateX(0); }
  .evo-layer-label { font-size: 12px; font-weight: 600; margin-bottom: 8px; display: flex; align-items: center; gap: 10px; }
  .evo-layer-name { color: var(--accent); font-family: var(--mono); min-width: 60px; }
  .evo-layer-desc { color: var(--text-3); font-size: 11px; font-style: italic; }
  .evo-bars { display: flex; gap: 3px; align-items: flex-end; height: 60px; }

  .evo-bar-col { display: flex; flex-direction: column; align-items: center; flex: 1; height: 100%; justify-content: flex-end; }
  .evo-bar { width: 100%; border-radius: 3px 3px 0 0; transition: height 0.8s ease; min-height: 1px; }
  .evo-bar-tok { font-family: var(--mono); font-size: 8px; color: var(--text-3); margin-top: 4px; writing-mode: vertical-rl; text-orientation: mixed; max-height: 50px; overflow: hidden; }
</style>
</head>
<body>

<div class="progress-bar" id="progressBar" style="width: 0%;"></div>

<div class="container">
  <h1>How jllm Learns</h1>
  <p class="subtitle">From raw text to neural network — a visual guide built from jllm's real trained weights</p>

  <!-- ═══════════════════════════════════════════ -->
  <!-- CHAPTER 1: TOKENIZATION (Scenes 0-4)       -->
  <!-- ═══════════════════════════════════════════ -->

  <div class="scene active" id="scene0">
    <div class="scene-label">Chapter 1: Tokenization — 1 of 5</div>
    <div class="scene-heading">This is your training data</div>
    <div class="scene-desc">One page from 370 data shards, pulled from FineWeb. This is the first document jllm will ever read. Just a random webpage about shipping logistics.</div>
    <div class="card card-glow"><div class="text-display" id="raw-text"></div></div>
    <div class="insight">
      <div class="insight-label">Insight</div>
      <p>Your 33GB of training data contains roughly 6.6 billion words from millions of web pages. Not curated. Not labeled. Just the internet. The model learns grammar, facts, and reasoning from this raw text alone.</p>
    </div>
  </div>

  <div class="scene" id="scene1">
    <div class="scene-label">Chapter 1: Tokenization — 2 of 5</div>
    <div class="scene-heading">But computers see numbers, not letters</div>
    <div class="scene-desc">Every character maps to a byte value (0-255). Hover over any character to see its number. This is all a computer sees: a long sequence of numbers.</div>
    <div class="card card-glow" style="padding-top: 36px;"><div class="text-display" id="byte-text"></div></div>
    <div class="stats">
      <div class="card stat-box stat-purple"><div class="stat-value">92</div><div class="stat-label">Characters</div><div class="stat-detail">one byte each</div></div>
      <div class="card stat-box stat-purple"><div class="stat-value">92</div><div class="stat-label">Sequence Length</div><div class="stat-detail">positions to process</div></div>
    </div>
    <div class="insight">
      <div class="insight-label">The Problem</div>
      <p>The model reads text through a fixed window of <strong>2,048 positions</strong>. At byte level, that is only ~330 words. The word "transportation" alone eats 14 positions. We need to compress.</p>
    </div>
  </div>

  <div class="scene" id="scene2">
    <div class="scene-label">Chapter 1: Tokenization — 3 of 5</div>
    <div class="scene-heading">BPE: Learning to merge</div>
    <div class="scene-desc">Byte Pair Encoding scans billions of characters and asks one question, over and over: <strong>"Which two adjacent pieces appear together most often?"</strong> Then it merges them. Watch it work on "the cat sat on the mat":</div>
    <div id="bpe-steps"></div>
    <div class="insight" style="margin-top: 24px;">
      <div class="insight-label">Insight</div>
      <p>BPE is a compression algorithm. Common words like "the" become single tokens fast. Rare words stay as smaller pieces. The jllm tokenizer did <strong>32,768 merges</strong> on 2 billion characters of real text. It never fails on unknown words because it can always fall back to individual bytes.</p>
    </div>
  </div>

  <div class="scene" id="scene3">
    <div class="scene-label">Chapter 1: Tokenization — 4 of 5</div>
    <div class="scene-heading">Your tokenizer: 32,768 merges later</div>
    <div class="scene-desc">After training on 2 billion characters, your tokenizer learned a vocabulary of 32,768 tokens. Common words become single tokens. Here is the same sentence, tokenized:</div>
    <div class="card card-glow"><div class="token-container" id="real-tokens"></div></div>
    <div class="stats">
      <div class="card stat-box stat-purple"><div class="stat-value">92</div><div class="stat-label">As Bytes</div></div>
      <div class="card stat-box stat-green"><div class="stat-value">14</div><div class="stat-label">As Tokens</div></div>
      <div class="card stat-box stat-blue"><div class="stat-value">6.6x</div><div class="stat-label">Compression</div></div>
    </div>
    <div class="insight">
      <div class="insight-label">Why This Matters</div>
      <p><strong>"transportation"</strong> is 14 bytes but just 1 token. The whole sentence compresses from 92 to 14 positions. The model's 2,048-token window now holds ~2,100 words instead of ~330. More context means better understanding.</p>
    </div>
  </div>

  <div class="scene" id="scene4">
    <div class="scene-label">Chapter 1: Tokenization — 5 of 5</div>
    <div class="scene-heading">This is what the model actually sees</div>
    <div class="scene-desc">Not letters. Not words. A sequence of token IDs. Each one becomes a <strong>1,664-dimensional vector</strong> as it enters the neural network.</div>
    <div class="card card-glow"><div class="ids-display" id="final-ids"></div></div>
    <div class="card" style="margin-top: 20px;">
      <h3 style="font-size:14px;color:var(--text-3);margin-bottom:16px;font-weight:500;">The model's 2,048-position window</h3>
      <div class="window-row">
        <div class="window-label" style="color:var(--accent);">Byte level</div>
        <div class="bar-outer"><div class="bar-inner" id="byte-bar" style="width:0%;background:linear-gradient(90deg,var(--accent),var(--accent2));"></div><div class="bar-text">~330 words</div></div>
      </div>
      <div class="window-row">
        <div class="window-label" style="color:var(--green);">Token level</div>
        <div class="bar-outer"><div class="bar-inner" id="token-bar" style="width:0%;background:linear-gradient(90deg,#22c55e,#16a34a);"></div><div class="bar-text">~2,100 words</div></div>
      </div>
    </div>
    <div class="insight" style="margin-top: 24px;">
      <div class="insight-label">The Journey So Far</div>
      <p>Text to Bytes to Tokens to IDs. But these are still just numbers. <strong>Next up:</strong> how each ID becomes a rich 1,664-dimensional vector through the embedding table.</p>
    </div>
  </div>

  <!-- ═══════════════════════════════════════════ -->
  <!-- CHAPTER 2: EMBEDDINGS (Scenes 5-9)          -->
  <!-- ═══════════════════════════════════════════ -->

  <div class="scene" id="scene5">
    <div class="chapter-break"><div class="chapter-num">Chapter 2</div><div class="chapter-title">Embeddings</div></div>
    <div class="scene-label">Chapter 2: Embeddings — 1 of 5</div>
    <div class="scene-heading">Every token gets a vector</div>
    <div class="scene-desc">The model stores a giant table: one row for each of its 32,768 tokens. Each row is a vector of <strong>1,664 numbers</strong>. When a token enters the model, its ID is used to look up the corresponding row. That vector becomes the token's identity inside the network.</div>
    <div class="stats">
      <div class="card stat-box stat-purple"><div class="stat-value">32,768</div><div class="stat-label">Vocabulary Size</div></div>
      <div class="card stat-box stat-purple"><div class="stat-value">1,664</div><div class="stat-label">Dimensions</div></div>
      <div class="card stat-box stat-blue"><div class="stat-value">54.5M</div><div class="stat-label">Parameters</div></div>
    </div>
    <div class="card card-glow" id="lookup-demo"></div>
    <div class="insight">
      <div class="insight-label">Insight</div>
      <p>The embedding table alone holds <strong>54.5 million numbers</strong> (32,768 x 1,664). That is 3.2% of jllm's 1.68 billion parameters, dedicated entirely to giving each token its starting identity. These values are learned during training. No human chose them.</p>
    </div>
  </div>

  <div class="scene" id="scene6">
    <div class="scene-label">Chapter 2: Embeddings — 2 of 5</div>
    <div class="scene-heading">Every word has a fingerprint</div>
    <div class="scene-desc">Here are the <strong>first 8 dimensions</strong> (out of 1,664) for 8 words from jllm's trained weights. Purple means positive, blue means negative, darker means closer to zero. Each word has a completely unique pattern.</div>
    <div class="card card-glow" id="heatmap-container"></div>
    <div class="insight">
      <div class="insight-label">Look Closely</div>
      <p>Compare <strong>"The" and "the"</strong>: their patterns are strikingly similar (both negative in dims 2-4, both positive in dim 5). That makes sense, they are the same word. Now compare <strong>"cat" and "dog"</strong>: their patterns are almost opposite. Dim 0 is +13 vs -12. Dim 5 is +33 vs -33. The model does not yet know they are related.</p>
    </div>
  </div>

  <div class="scene" id="scene7">
    <div class="scene-label">Chapter 2: Embeddings — 3 of 5</div>
    <div class="scene-heading">The similarity surprise</div>
    <div class="scene-desc">Cosine similarity measures how much two vectors point in the same direction. A value of 1.0 means identical, 0.0 means unrelated, and negative means opposing. Look what happens when we measure similarity in the raw embedding table:</div>
    <div class="card card-glow" id="sim-chart"></div>
    <div class="insight">
      <div class="insight-label">Why Is This Surprising?</div>
      <p>Cat and dog have <strong>zero similarity</strong>. Man and woman are slightly <strong>negative</strong>. Good and great are <strong>opposites</strong>. Only "The" and "the" show real similarity (0.77), because they are literally the same word with different capitalization. These are the <strong>raw input embeddings</strong>: a learned coordinate system, not a map of meaning. The semantic understanding emerges through the 26 transformer layers and attention. The embedding table is where the journey <strong>begins</strong>, not where meaning lives.</p>
    </div>
  </div>

  <div class="scene" id="scene8">
    <div class="scene-label">Chapter 2: Embeddings — 4 of 5</div>
    <div class="scene-heading">The landscape of vectors</div>
    <div class="scene-desc">We can compress 1,664 dimensions down to 2 using PCA (Principal Component Analysis) to see the layout. In word2vec, you would see tight clusters: animals together, colors together. In transformer input embeddings, the landscape is more scattered. <strong>The clustering happens deeper in the network.</strong></div>
    <div class="card card-glow pca-wrap"><canvas id="pca-canvas" class="pca-canvas"></canvas></div>
    <div class="insight">
      <div class="insight-label">What This Means</div>
      <p>The embedding table is like a <strong>random starting position</strong> for each token. During training, the model learns to arrange tokens in this 1,664-dimensional space so that the transformer layers can most effectively process them. The arrangement is not about "meaning" but about what makes prediction easiest. The 26 transformer layers take these scattered vectors and, through attention, create rich representations where "cat" and "dog" finally become related.</p>
    </div>
  </div>

  <div class="scene" id="scene9">
    <div class="scene-label">Chapter 2: Embeddings — 5 of 5</div>
    <div class="scene-heading">The pipeline so far</div>
    <div class="scene-desc">You have now seen the full journey from raw text to the vectors that enter the neural network. Two chapters down. The biggest piece is still ahead: <strong>what happens inside the transformer layers?</strong></div>
    <div class="card card-glow" id="pipeline-container"></div>
    <div class="insight">
      <div class="insight-label">Coming Next: Attention</div>
      <p>The transformer's secret weapon is <strong>attention</strong>: a mechanism that lets every token look at every other token and decide what information to absorb. Through 26 rounds of this, the scattered embedding vectors transform into rich, contextualized representations. The word "bank" next to "river" ends up in a completely different place than "bank" next to "money." That is where the real magic happens.</p>
    </div>
  </div>

  <!-- ═══════════════════════════════════════════ -->
  <!-- CHAPTER 3: ATTENTION (Scenes 10-14)        -->
  <!-- ═══════════════════════════════════════════ -->

  <div class="scene" id="scene10">
    <div class="chapter-break"><div class="chapter-num">Chapter 3</div><div class="chapter-title">Attention</div></div>
    <div class="scene-label">Chapter 3: Attention — 1 of 5</div>
    <div class="scene-heading">Every token talks to every token</div>
    <div class="scene-desc">Remember how the raw embeddings for "cat" and "dog" were completely unrelated (cosine similarity = 0.0)? That is because embeddings are <strong>context-free</strong> — each token gets the same vector regardless of its neighbors. Attention fixes this. It lets each token look at every previous token and decide: <strong>"How much should I pay attention to you?"</strong></div>
    <div class="card card-glow" id="attn-arrows-container"></div>
    <div class="insight">
      <div class="insight-label">What you are seeing</div>
      <p>This is <strong>real data from jllm's Layer 0, Head 4</strong>. The token "·shipment" attends to "·transportation" with weight 0.341 — they are topically related. It also attends strongly to itself (0.443). Notice the attention is <strong>causal</strong>: each token can only look at tokens that came before it (or itself). It never peeks ahead.</p>
    </div>
  </div>

  <div class="scene" id="scene11">
    <div class="scene-label">Chapter 3: Attention — 2 of 5</div>
    <div class="scene-heading">The attention heatmap</div>
    <div class="scene-desc">Here is the full picture: a <strong>14×14 attention matrix</strong> from jllm's Layer 0, Head 0. Each row is a token asking "who should I listen to?" Each column is a token answering. Brighter = more attention. Hover to inspect.</div>
    <div class="card card-glow" id="attn-heatmap-container"></div>
    <div class="insight">
      <div class="insight-label">Reading the Heatmap</div>
      <p>See the bright column at "·important"? Almost every token after it pays attention to this word — it is a <strong>content anchor</strong>. The triangle of black in the upper-right is the causal mask: future tokens are blocked. Also notice the bright diagonal — tokens always attend somewhat to themselves. This is one of 13 heads, each learning a different pattern.</p>
    </div>
  </div>

  <div class="scene" id="scene12">
    <div class="scene-label">Chapter 3: Attention — 3 of 5</div>
    <div class="scene-heading">13 heads, 13 jobs</div>
    <div class="scene-desc">jllm has <strong>13 attention heads per layer</strong>, each with its own learned weights. They all process the same input but learn completely different patterns. Here is what each head focuses on when processing the token "·shipment" in Layer 0:</div>
    <div id="multihead-container"></div>
    <div class="insight">
      <div class="insight-label">Head Specialization</div>
      <p>Each head has its own Q, K, V weight matrices — <strong>128 dimensions each</strong>. Head 6 became a "copy head" (65% self-attention). Head 4 found topic relationships. Head 0 found content anchors. Nobody programmed these roles — they emerged from training on 33GB of text. After all 13 heads compute their outputs, the results are concatenated (13 × 128 = 1,664) and projected back. Thirteen perspectives fused into one.</p>
    </div>
  </div>

  <div class="scene" id="scene13">
    <div class="scene-label">Chapter 3: Attention — 4 of 5</div>
    <div class="scene-heading">Through the layers</div>
    <div class="scene-desc">jllm stacks <strong>26 attention layers</strong>, each refining the representations. Watch how "·shipment" shifts what it attends to as information flows deeper into the network (Head 0 across 5 layers):</div>
    <div class="card card-glow" id="evolution-container"></div>
    <div class="insight">
      <div class="insight-label">The Journey</div>
      <p><strong>Layer 0:</strong> "·shipment" looks at "·important" — gathering semantic content. <strong>Layer 12:</strong> It locks onto itself — consolidating its identity. <strong>Layer 25:</strong> It distributes attention to "·is" and "·when" — grammatical structure words. The model first asks "what is this about?", then "what am I?", then "how do I fit in the sentence?" This is how 1,664-dim vectors go from random embeddings to rich, contextualized representations.</p>
    </div>
  </div>

  <div class="scene" id="scene14">
    <div class="scene-label">Chapter 3: Attention — 5 of 5</div>
    <div class="scene-heading">The pipeline so far</div>
    <div class="scene-desc">Three chapters in. You have seen jllm go from raw bytes to tokens to embeddings to contextualized representations through 26 layers of multi-head attention. Here is the full pipeline:</div>
    <div class="card card-glow" id="pipeline-container-2"></div>
    <div class="stats">
      <div class="card stat-box stat-purple"><div class="stat-value">26</div><div class="stat-label">Layers</div><div class="stat-detail">stacked transformers</div></div>
      <div class="card stat-box stat-purple"><div class="stat-value">13</div><div class="stat-label">Heads / Layer</div><div class="stat-detail">parallel perspectives</div></div>
      <div class="card stat-box stat-blue"><div class="stat-value">338</div><div class="stat-label">Total Heads</div><div class="stat-detail">26 × 13</div></div>
    </div>
    <div class="insight">
      <div class="insight-label">Coming Next</div>
      <p>Attention is only half of each transformer layer. The other half is the <strong>MLP (feed-forward network)</strong> — a dense network that processes each token independently, storing factual knowledge and learned patterns. Together, attention + MLP × 26 layers transforms raw token embeddings into a representation so rich that the model can predict the next token with startling accuracy. That is the final piece of the puzzle.</p>
    </div>
  </div>

  <div class="spacer"></div>
</div>

<div class="controls">
  <button class="btn btn-secondary" id="prevBtn" disabled>Back</button>
  <div class="step-indicator" id="stepIndicator">Scene 1 of 10</div>
  <button class="btn btn-primary" id="nextBtn">Next</button>
</div>

<script>
/* ── Tokenizer Data ── */
var SENTENCE = "The mode of transportation is an important consideration when planning the shipment process.";
var TOKENS = [
  {text:"The",id:449},{text:"\u00b7mode",id:6938},{text:"\u00b7of",id:281},
  {text:"\u00b7transportation",id:6244},{text:"\u00b7is",id:309},{text:"\u00b7an",id:351},
  {text:"\u00b7important",id:952},{text:"\u00b7consideration",id:7937},{text:"\u00b7when",id:626},
  {text:"\u00b7planning",id:3921},{text:"\u00b7the",id:261},{text:"\u00b7shipment",id:32330},
  {text:"\u00b7process",id:937},{text:".",id:46}
];

/* ── Embeddings Data (extracted from jllm's trained weights) ── */
var WORDS = [
  {w:"The",id:449,d:[6.62,7.5,-21.0,-25.38,-11.19,22.75,35.0,-21.25]},
  {w:"the",id:261,d:[6.12,1.62,-26.0,-20.0,-15.06,30.0,22.75,-37.0]},
  {w:"cat",id:14261,d:[13.0,4.38,34.5,-7.44,-14.12,33.25,7.0,-21.12]},
  {w:"dog",id:9499,d:[-12.0,0.06,24.5,14.19,-6.16,-33.25,-10.06,12.94]},
  {w:"man",id:493,d:[-17.62,4.03,-9.56,-6.75,6.12,-7.72,10.56,1.73]},
  {w:"woman",id:3400,d:[10.56,1.68,3.69,9.94,22.12,-1.09,-13.0,1.09]},
  {w:"good",id:8930,d:[7.53,-0.86,-7.78,-13.62,8.94,-34.0,-0.42,19.12]},
  {w:"bad",id:4430,d:[11.5,10.44,-2.03,18.5,1.52,-15.88,-21.12,-3.86]}
];
var SIMS = [
  {w1:"The",w2:"the",s:0.770},{w1:"new",w2:"old",s:0.132},
  {w1:"good",w2:"bad",s:0.094},{w1:"king",w2:"man",s:0.093},
  {w1:"day",w2:"work",s:0.093},{w1:"love",w2:"food",s:0.087},
  {w1:"big",w2:"small",s:0.072},{w1:"water",w2:"food",s:0.056},
  {w1:"black",w2:"white",s:0.038},{w1:"cat",w2:"work",s:0.028},
  {w1:"cat",w2:"king",s:0.019},{w1:"cat",w2:"dog",s:0.000},
  {w1:"man",w2:"woman",s:-0.013},{w1:"red",w2:"black",s:-0.027},
  {w1:"good",w2:"great",s:-0.053}
];
var PCA = [
  {w:"The",x:-238.94,y:-68.58},{w:"the",x:-152.74,y:-30.17},
  {w:"cat",x:-75.4,y:-131.01},{w:"dog",x:245.66,y:-57.95},
  {w:"man",x:-219.26,y:54.39},{w:"woman",x:116.7,y:-72.23},
  {w:"good",x:45.5,y:-126.16},{w:"great",x:-59.27,y:-364.58},
  {w:"bad",x:329.0,y:-9.06},{w:"big",x:95.26,y:31.42},
  {w:"small",x:-114.06,y:48.41},{w:"work",x:155.09,y:-53.87},
  {w:"day",x:-252.68,y:-224.65},{w:"new",x:-56.93,y:116.84},
  {w:"old",x:-177.55,y:114.38},{w:"black",x:-14.24,y:153.14},
  {w:"white",x:334.44,y:25.44},{w:"red",x:-90.99,y:344.92},
  {w:"king",x:134.02,y:-179.04},{w:"water",x:234.86,y:105.93},
  {w:"food",x:-0.78,y:53.87},{w:"love",x:-15.87,y:274.47},
  {w:"is",x:-122.42,y:4.49},{w:"of",x:-99.4,y:-10.4}
];

/* ── Navigation ── */
var cur = 0, total = 15;
function $(id) { return document.getElementById(id); }
var sceneFns = [tokS1,tokS2,tokS3,tokS4,tokS5,embS1,embS2,embS3,embS4,embS5,attS1,attS2,attS3,attS4,attS5];

function showScene(n) {
  for (var i = 0; i < total; i++) { var s = $("scene"+i); s.classList.remove("active","visible"); }
  var sc = $("scene"+n); sc.classList.add("active");
  setTimeout(function(){ sc.classList.add("visible"); }, 50);
  sceneFns[n]();
  $("prevBtn").disabled = (n===0);
  $("nextBtn").textContent = (n===total-1) ? "Restart" : "Next";
  $("stepIndicator").textContent = "Scene "+(n+1)+" of "+total;
  $("progressBar").style.width = ((n+1)/total*100)+"%";
  window.scrollTo({top:0,behavior:"smooth"});
}

$("nextBtn").addEventListener("click", function(){
  if (cur<total-1){cur++;showScene(cur);} else {cur=0;showScene(0);}
});
$("prevBtn").addEventListener("click", function(){
  if (cur>0){cur--;showScene(cur);}
});
document.addEventListener("keydown", function(e){
  if (e.target.tagName==="INPUT"||e.target.tagName==="TEXTAREA") return;
  if (e.key==="ArrowRight"||e.key===" "){e.preventDefault();$("nextBtn").click();}
  if (e.key==="ArrowLeft"){e.preventDefault();$("prevBtn").click();}
});

/* ══════════════════════════════════════════ */
/* CHAPTER 1: TOKENIZATION SCENES            */
/* ══════════════════════════════════════════ */

function tokS1() {
  var t = $("raw-text"); t.textContent = ""; var i = 0;
  (function type(){ if(i<SENTENCE.length){ t.textContent=SENTENCE.substring(0,i+1); i++; setTimeout(type,25); } })();
}

function tokS2() {
  var t = $("byte-text"); t.textContent = "";
  for (var i=0; i<SENTENCE.length; i++) {
    var ch=SENTENCE[i], code=SENTENCE.charCodeAt(i);
    var span=document.createElement("span"); span.className="char-box";
    var lbl=document.createElement("span"); lbl.className="byte-val"; lbl.textContent=code;
    span.appendChild(lbl);
    span.appendChild(document.createTextNode(ch===" "?"\u00a0":ch));
    (function(s){ s.addEventListener("mouseenter",function(){s.classList.add("show-bytes","highlight");}); s.addEventListener("mouseleave",function(){s.classList.remove("show-bytes","highlight");}); })(span);
    t.appendChild(span);
  }
  var boxes=t.querySelectorAll(".char-box");
  for (var j=0; j<Math.min(14,boxes.length); j++) {
    (function(b,d){ setTimeout(function(){ b.classList.add("show-bytes","highlight"); setTimeout(function(){b.classList.remove("highlight");},800); },500+d*120); })(boxes[j],j);
  }
}

function tokS3() {
  var c = $("bpe-steps"); c.textContent = "";
  var merges = [
    {step:1,o1:"a",o2:"t",res:"at",cnt:"3 times",b:22,a:19,toks:["t","h","e","_","c",{m:"at"},"_","s",{m:"at"},"_","o","n","_","t","h","e","_","m",{m:"at"}]},
    {step:2,o1:"t",o2:"h",res:"th",cnt:"2 times",b:19,a:17,toks:[{m:"th"},"e","_","c",{m:"at"},"_","s",{m:"at"},"_","o","n","_",{m:"th"},"e","_","m",{m:"at"}]},
    {step:3,o1:"th",o2:"e",res:"the",cnt:"2 times",b:17,a:15,toks:[{m:"the"},"_","c",{m:"at"},"_","s",{m:"at"},"_","o","n","_",{m:"the"},"_","m",{m:"at"}]},
    {step:4,o1:"s",o2:"at",res:"sat",cnt:"1 time",b:15,a:14,toks:[{m:"the"},"_","c",{m:"at"},"_",{m:"sat"},"_","o","n","_",{m:"the"},"_","m",{m:"at"}]},
    {step:5,o1:"c",o2:"at",res:"cat",cnt:"1 time",b:14,a:13,toks:[{m:"the"},"_",{m:"cat"},"_",{m:"sat"},"_","o","n","_",{m:"the"},"_","m",{m:"at"}]},
    {step:6,o1:"m",o2:"at",res:"mat",cnt:"1 time",b:13,a:11,toks:[{m:"the"},"_",{m:"cat"},"_",{m:"sat"},"_","o","n","_",{m:"the"},"_",{m:"mat"}]}
  ];
  merges.forEach(function(mg,idx){
    var div=document.createElement("div"); div.className="merge-step";
    var sn=document.createElement("div"); sn.className="step-num"; sn.textContent="Merge #"+mg.step; div.appendChild(sn);
    var pair=document.createElement("div"); pair.className="merge-pair";
    var old=document.createElement("span"); old.className="merge-old"; old.textContent='"'+mg.o1+'" + "'+mg.o2+'"'; pair.appendChild(old);
    var arrow=document.createElement("span"); arrow.className="merge-arrow"; arrow.textContent="\u2192"; pair.appendChild(arrow);
    var nw=document.createElement("span"); nw.className="merge-new"; nw.textContent='"'+mg.res+'"'; pair.appendChild(nw);
    var info=document.createElement("span"); info.className="merge-info"; info.textContent="("+mg.cnt+")"; pair.appendChild(info);
    div.appendChild(pair);
    var cnt=document.createElement("div"); cnt.className="merge-count"; cnt.textContent=mg.b+" tokens \u2192 "+mg.a+" tokens"; div.appendChild(cnt);
    var res=document.createElement("div"); res.className="merge-result";
    mg.toks.forEach(function(tok,ti){
      var sp=document.createElement("span");
      if(typeof tok==="string"){sp.textContent=tok;} else {sp.textContent=tok.m;sp.className="merged";}
      res.appendChild(sp); if(ti<mg.toks.length-1) res.appendChild(document.createTextNode(" "));
    });
    div.appendChild(res); c.appendChild(div);
    setTimeout(function(){div.classList.add("visible");},300+idx*500);
  });
}

function tokS4() {
  var c=$("real-tokens"); c.textContent="";
  var colors=["tc0","tc1","tc2","tc3","tc4","tc5","tc6","tc7"];
  TOKENS.forEach(function(tok,idx){
    var div=document.createElement("div"); div.className="token-box "+colors[idx%colors.length];
    var txt=document.createElement("div"); txt.className="token-text"; txt.textContent=tok.text;
    var tid=document.createElement("div"); tid.className="token-id"; tid.textContent=tok.id;
    div.appendChild(txt); div.appendChild(tid); c.appendChild(div);
    setTimeout(function(){div.classList.add("visible");},200+idx*150);
  });
}

function tokS5() {
  var t=$("final-ids"); t.textContent="";
  var colors=["#a855f7","#ec4899","#3b82f6","#f59e0b","#22c55e","#06b6d4","#f43f5e","#8b5cf6"];
  TOKENS.forEach(function(tok,idx){
    var sp=document.createElement("span"); sp.className="id-span";
    sp.style.color=colors[idx%colors.length]; sp.style.opacity="0";
    sp.textContent=tok.id+(idx<TOKENS.length-1?"  ":"");
    t.appendChild(sp);
    setTimeout(function(){sp.style.opacity="1";},300+idx*200);
  });
  setTimeout(function(){$("byte-bar").style.width="15%";$("token-bar").style.width="100%";},800);
}

/* ══════════════════════════════════════════ */
/* CHAPTER 2: EMBEDDINGS SCENES              */
/* ══════════════════════════════════════════ */

function embS1() {
  var box=$("lookup-demo"); box.textContent="";
  var show3=[WORDS[2],WORDS[3],WORDS[4]];
  show3.forEach(function(w,wi){
    var row=document.createElement("div"); row.className="lookup-row";
    row.style.opacity="0"; row.style.transform="translateY(12px)";
    row.style.transition="all 0.6s ease "+(wi*0.4)+"s";
    var word=document.createElement("span"); word.className="lookup-word"; word.textContent='"'+w.w+'"'; row.appendChild(word);
    var a1=document.createElement("span"); a1.className="lookup-arrow"; a1.textContent="\u2192"; row.appendChild(a1);
    var id=document.createElement("span"); id.className="lookup-id"; id.textContent="ID "+w.id; row.appendChild(id);
    var a2=document.createElement("span"); a2.className="lookup-arrow"; a2.textContent="\u2192"; row.appendChild(a2);
    var vec=document.createElement("span"); vec.className="lookup-vec";
    vec.textContent="["+w.d.map(function(v){return v.toFixed(1);}).join(", ")+", ... 1656 more]";
    row.appendChild(vec); box.appendChild(row);
    setTimeout(function(){row.style.opacity="1";row.style.transform="translateY(0)";},100);
  });
}

function valToColor(v) {
  var cap=45,n=Math.max(-1,Math.min(1,v/cap));
  if(n<0){var t=Math.abs(n);return "rgba(59,130,246,"+(t*0.75+0.08)+")";}
  return "rgba(168,85,247,"+(n*0.75+0.08)+")";
}

function embS2() {
  var box=$("heatmap-container"); box.textContent="";
  var hm=document.createElement("div"); hm.className="heatmap";
  var hdr=document.createElement("div"); hdr.className="heat-row";
  var blank=document.createElement("div"); blank.className="heat-label"; hdr.appendChild(blank);
  for(var d=0;d<8;d++){
    var dl=document.createElement("div"); dl.className="heat-cell heat-dim-label";
    dl.textContent="dim "+d; dl.style.background="none"; hdr.appendChild(dl);
  }
  hm.appendChild(hdr);
  WORDS.forEach(function(w,wi){
    var row=document.createElement("div"); row.className="heat-row";
    row.style.opacity="0"; row.style.transition="opacity 0.5s ease "+(wi*0.15)+"s";
    var label=document.createElement("div"); label.className="heat-label"; label.textContent=w.w; row.appendChild(label);
    w.d.forEach(function(v){
      var cell=document.createElement("div"); cell.className="heat-cell";
      cell.style.background=valToColor(v);
      cell.textContent=v>0?"+"+v.toFixed(0):v.toFixed(0);
      cell.title=w.w+": "+v.toFixed(2); row.appendChild(cell);
    });
    hm.appendChild(row);
    setTimeout(function(){row.style.opacity="1";},100);
  });
  var note=document.createElement("div"); note.className="heat-bracket";
  note.textContent="\u2190 8 of 1,664 dimensions shown \u2192";
  hm.appendChild(note); box.appendChild(hm);
}

function embS3() {
  var box=$("sim-chart"); box.textContent="";
  SIMS.forEach(function(s,si){
    var row=document.createElement("div"); row.className="sim-row";
    if(si===0) row.classList.add("sim-highlight");
    row.style.opacity="0"; row.style.transform="translateX(-10px)";
    row.style.transition="all 0.4s ease "+(si*0.12)+"s";
    var label=document.createElement("div"); label.className="sim-label";
    label.textContent=s.w1+" / "+s.w2; row.appendChild(label);
    var barWrap=document.createElement("div"); barWrap.className="sim-bar-wrap";
    var bar=document.createElement("div"); bar.className="sim-bar";
    var pct=Math.max(0,s.s/0.77)*100;
    bar.style.width="0%";
    if(s.s>=0){bar.style.background="linear-gradient(90deg,#a855f7,#ec4899)";}
    else{bar.style.background="#3b82f6";pct=Math.abs(s.s)/0.77*100;}
    barWrap.appendChild(bar); row.appendChild(barWrap);
    var val=document.createElement("div"); val.className="sim-val "+(s.s>=0?"sim-pos":"sim-neg");
    val.textContent=(s.s>=0?"+":"")+s.s.toFixed(3); row.appendChild(val);
    box.appendChild(row);
    setTimeout(function(){row.style.opacity="1";row.style.transform="translateX(0)";bar.style.width=pct+"%";},200);
  });
}

function embS4() {
  var canvas=$("pca-canvas");
  var dpr=window.devicePixelRatio||1;
  var W=856,H=520;
  canvas.width=W*dpr; canvas.height=H*dpr;
  canvas.style.width=W+"px"; canvas.style.height=H+"px";
  var ctx=canvas.getContext("2d"); ctx.scale(dpr,dpr);
  var pad=50,minX=-280,maxX=360,minY=-390,maxY=370;
  function mx(x){return pad+((x-minX)/(maxX-minX))*(W-2*pad);}
  function my(y){return H-pad-((y-minY)/(maxY-minY))*(H-2*pad);}

  ctx.fillStyle="#111"; ctx.fillRect(0,0,W,H);
  ctx.strokeStyle="#1a1a1a"; ctx.lineWidth=1;
  for(var gx=-200;gx<=300;gx+=100){ctx.beginPath();ctx.moveTo(mx(gx),pad);ctx.lineTo(mx(gx),H-pad);ctx.stroke();}
  for(var gy=-300;gy<=300;gy+=100){ctx.beginPath();ctx.moveTo(pad,my(gy));ctx.lineTo(W-pad,my(gy));ctx.stroke();}
  ctx.strokeStyle="#2a2a2a";
  ctx.beginPath();ctx.moveTo(mx(minX),my(0));ctx.lineTo(mx(maxX),my(0));ctx.stroke();
  ctx.beginPath();ctx.moveTo(mx(0),my(minY));ctx.lineTo(mx(0),my(maxY));ctx.stroke();
  ctx.fillStyle="#555";ctx.font="11px Inter";ctx.textAlign="center";
  ctx.fillText("PC1",W/2,H-12);
  ctx.save();ctx.translate(14,H/2);ctx.rotate(-Math.PI/2);ctx.fillText("PC2",0,0);ctx.restore();

  PCA.forEach(function(p,i){
    setTimeout(function(){
      var px=mx(p.x),py=my(p.y);
      var grad=ctx.createRadialGradient(px,py,0,px,py,16);
      grad.addColorStop(0,"rgba(168,85,247,0.3)");grad.addColorStop(1,"rgba(168,85,247,0)");
      ctx.fillStyle=grad; ctx.beginPath();ctx.arc(px,py,16,0,Math.PI*2);ctx.fill();
      ctx.beginPath();ctx.arc(px,py,4.5,0,Math.PI*2);ctx.fillStyle="#a855f7";ctx.fill();
      ctx.fillStyle="#e0e0e0";ctx.font="600 12px Inter";ctx.textAlign="center";
      ctx.fillText(p.w,px,py-11);
    },i*80);
  });
}

function embS5() {
  var box=$("pipeline-container"); box.textContent="";
  var pipe=document.createElement("div"); pipe.className="pipeline";
  var steps=[
    {name:"Raw Text",detail:"Training data",status:"done",cls:"pipe-done"},
    {name:"Tokenizer",detail:"BPE, 32K vocab",status:"done",cls:"pipe-done"},
    {name:"Token IDs",detail:"[449, 6938, ...]",status:"done",cls:"pipe-done"},
    {name:"Embeddings",detail:"32,768 \u00d7 1,664",status:"now",cls:"pipe-current"},
    {name:"Vectors",detail:"1,664-dim each",status:"now",cls:"pipe-current"},
    {name:"Transformer",detail:"\u00d726 layers",status:"next",cls:"pipe-next"},
    {name:"Prediction",detail:"Next token",status:"next",cls:"pipe-next"}
  ];
  steps.forEach(function(st,si){
    if(si>0){var arrow=document.createElement("div");arrow.className="pipe-arrow";arrow.textContent="\u2192";pipe.appendChild(arrow);}
    var step=document.createElement("div"); step.className="pipe-step "+st.cls;
    step.style.opacity="0"; step.style.transition="opacity 0.5s ease "+(si*0.2)+"s";
    var name=document.createElement("div");name.className="pipe-name";name.textContent=st.name;step.appendChild(name);
    var detail=document.createElement("div");detail.className="pipe-detail";detail.textContent=st.detail;step.appendChild(detail);
    var status=document.createElement("div");
    var scls=st.status==="done"?"pipe-status-done":(st.status==="now"?"pipe-status-now":"pipe-status-next");
    status.className="pipe-status "+scls;
    status.textContent=st.status==="done"?"Chapter 1":(st.status==="now"?"Chapter 2":"Up Next");
    step.appendChild(status); pipe.appendChild(step);
    setTimeout(function(){step.style.opacity="1";},100);
  });
  box.appendChild(pipe);
}

/* ══════════════════════════════════════════ */
/* CHAPTER 3: ATTENTION SCENES               */
/* ══════════════════════════════════════════ */

/* ── Attention Data (extracted from jllm's real Layer 0-25 weights) ── */
/* Model: 26 layers, 13 heads, head_dim=128, n_embd=1664 */
var ATT_TOKENS = [
  {text:"The",id:449},{text:"\u00b7mode",id:6938},{text:"\u00b7of",id:281},
  {text:"\u00b7transportation",id:6244},{text:"\u00b7is",id:309},{text:"\u00b7an",id:351},
  {text:"\u00b7important",id:952},{text:"\u00b7consideration",id:7937},{text:"\u00b7when",id:626},
  {text:"\u00b7planning",id:3921},{text:"\u00b7the",id:261},{text:"\u00b7shipment",id:32330},
  {text:"\u00b7process",id:937},{text:".",id:46}
];

/* Layer 0, Head 4: "shipment" attention row (topic head) */
var ARROW_DATA = [0.021,0.02,0.018,0.341,0.017,0.028,0.003,0.006,0.012,0.002,0.09,0.443,0.0,0.0];

/* Full 14x14 heatmap: Layer 0, Head 0 (content head) */
var HEATMAP = [
  [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.075,0.925,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.322,0.564,0.114,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.018,0.332,0.115,0.535,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.123,0.182,0.062,0.521,0.112,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.094,0.265,0.019,0.43,0.073,0.118,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.257,0.021,0.013,0.124,0.241,0.159,0.184,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.011,0.034,0.006,0.254,0.046,0.046,0.598,0.004,0.0,0.0,0.0,0.0,0.0,0.0],
  [0.011,0.02,0.002,0.045,0.093,0.041,0.688,0.063,0.036,0.0,0.0,0.0,0.0,0.0],
  [0.017,0.013,0.005,0.451,0.037,0.054,0.336,0.041,0.018,0.028,0.0,0.0,0.0,0.0],
  [0.026,0.011,0.008,0.163,0.083,0.074,0.345,0.018,0.13,0.131,0.011,0.0,0.0,0.0],
  [0.013,0.019,0.006,0.196,0.073,0.115,0.453,0.009,0.06,0.035,0.002,0.02,0.0,0.0],
  [0.003,0.032,0.003,0.144,0.015,0.032,0.451,0.046,0.015,0.204,0.002,0.03,0.021,0.0],
  [0.001,0.001,0.002,0.015,0.016,0.022,0.245,0.036,0.015,0.047,0.001,0.016,0.005,0.577]
];

/* All 13 heads - what "shipment" (idx 11) attends to in Layer 0 */
var MULTIHEAD = {
  0:[0.013,0.019,0.006,0.196,0.073,0.115,0.453,0.009,0.06,0.035,0.002,0.02,0.0,0.0],
  1:[0.068,0.018,0.005,0.013,0.174,0.04,0.009,0.025,0.182,0.019,0.441,0.005,0.0,0.0],
  2:[0.173,0.026,0.061,0.164,0.021,0.099,0.01,0.03,0.011,0.233,0.102,0.069,0.0,0.0],
  3:[0.038,0.012,0.012,0.039,0.16,0.195,0.038,0.004,0.001,0.002,0.021,0.478,0.0,0.0],
  4:[0.021,0.02,0.018,0.341,0.017,0.028,0.003,0.006,0.012,0.002,0.09,0.443,0.0,0.0],
  5:[0.076,0.027,0.033,0.02,0.04,0.025,0.051,0.071,0.22,0.231,0.14,0.068,0.0,0.0],
  6:[0.023,0.008,0.013,0.007,0.054,0.094,0.049,0.007,0.001,0.001,0.098,0.645,0.0,0.0],
  7:[0.015,0.142,0.02,0.071,0.076,0.012,0.076,0.217,0.061,0.185,0.008,0.118,0.0,0.0],
  8:[0.021,0.018,0.02,0.012,0.026,0.084,0.051,0.08,0.102,0.056,0.317,0.213,0.0,0.0],
  9:[0.037,0.1,0.084,0.087,0.165,0.084,0.028,0.015,0.013,0.01,0.164,0.213,0.0,0.0],
  10:[0.072,0.056,0.007,0.039,0.051,0.103,0.11,0.041,0.01,0.097,0.082,0.332,0.0,0.0],
  11:[0.058,0.009,0.043,0.024,0.066,0.073,0.05,0.059,0.073,0.289,0.166,0.09,0.0,0.0],
  12:[0.232,0.122,0.025,0.195,0.015,0.013,0.046,0.079,0.061,0.133,0.005,0.074,0.0,0.0]
};
var HEAD_ROLES = [
  "Content anchor","Structure","Spread","Self + grammar","Topic finder",
  "Temporal context","Copy head","Broad scan","Neighbor","Distributed",
  "Self + content","Planning focus","Lookback"
];

/* Evolution: "shipment" attention across 5 layers (Head 0) */
var EVOLUTION = {
  layer_0: [0.013,0.019,0.006,0.196,0.073,0.115,0.453,0.009,0.06,0.035,0.002,0.02,0.0,0.0],
  layer_6: [0.051,0.009,0.06,0.012,0.052,0.032,0.008,0.025,0.187,0.073,0.269,0.221,0.0,0.0],
  layer_12:[0.022,0.141,0.007,0.022,0.075,0.013,0.012,0.056,0.15,0.035,0.045,0.422,0.0,0.0],
  layer_18:[0.021,0.022,0.014,0.038,0.022,0.051,0.092,0.041,0.027,0.029,0.023,0.619,0.0,0.0],
  layer_25:[0.03,0.063,0.016,0.096,0.219,0.037,0.042,0.1,0.191,0.108,0.043,0.055,0.0,0.0]
};
var EVO_LABELS = [
  {name:"Layer 0",desc:"Semantic exploration"},
  {name:"Layer 6",desc:"Building context"},
  {name:"Layer 12",desc:"Finding identity"},
  {name:"Layer 18",desc:"Self-reinforcement"},
  {name:"Layer 25",desc:"Structural convergence"}
];

/* ── Scene 11: Attention Arrows ── */
function attS1() {
  var box = $("attn-arrows-container");
  box.textContent = "";
  var wrap = document.createElement("div");
  wrap.className = "attn-arrow-container";

  /* Source token label */
  var srcLabel = document.createElement("div");
  srcLabel.style.cssText = "text-align:center;margin-bottom:16px;font-size:13px;color:var(--text-3);";
  srcLabel.textContent = "Token \"\u00b7shipment\" attends to:";
  wrap.appendChild(srcLabel);

  /* SVG for arrows */
  var svgNS = "http://www.w3.org/2000/svg";
  var svg = document.createElementNS(svgNS, "svg");
  svg.setAttribute("width", "100%");
  svg.setAttribute("height", "160");
  svg.style.cssText = "display:block;overflow:visible;";
  wrap.appendChild(svg);

  /* Token row */
  var row = document.createElement("div");
  row.className = "attn-token-row";
  row.style.marginTop = "8px";

  var tokenEls = [];
  ATT_TOKENS.forEach(function(tok, i) {
    var el = document.createElement("div");
    el.className = "attn-token" + (i === 11 ? " source" : "");
    el.textContent = tok.text;
    el.style.opacity = "0";
    el.style.transition = "opacity 0.4s ease " + (i * 0.06) + "s";
    if (ARROW_DATA[i] > 0.05) {
      var wl = document.createElement("span");
      wl.className = "attn-weight-label";
      wl.textContent = ARROW_DATA[i].toFixed(3);
      wl.style.opacity = "0";
      wl.style.transition = "opacity 0.5s ease " + (0.8 + i * 0.06) + "s";
      el.appendChild(wl);
      tokenEls.push({el: el, wl: wl, idx: i});
    }
    row.appendChild(el);
    setTimeout(function() { el.style.opacity = "1"; }, 50);
  });
  wrap.appendChild(row);
  box.appendChild(wrap);

  /* Draw arrows after layout */
  setTimeout(function() {
    var boxRect = wrap.getBoundingClientRect();
    var sourceEl = row.children[11];
    var sourceRect = sourceEl.getBoundingClientRect();
    var sx = sourceRect.left + sourceRect.width / 2 - boxRect.left;
    var sy = sourceRect.top - boxRect.top;

    ATT_TOKENS.forEach(function(tok, i) {
      if (i === 11 || ARROW_DATA[i] < 0.01) return;
      var targetEl = row.children[i];
      var targetRect = targetEl.getBoundingClientRect();
      var tx = targetRect.left + targetRect.width / 2 - boxRect.left;
      var ty = targetRect.top - boxRect.top;

      var weight = ARROW_DATA[i];
      var opacity = Math.max(0.15, weight);
      var strokeW = Math.max(1, weight * 8);

      /* Curved path */
      var midY = sy - 40 - weight * 100;
      var path = document.createElementNS(svgNS, "path");
      var d = "M " + sx + " " + sy + " Q " + ((sx + tx) / 2) + " " + midY + " " + tx + " " + ty;
      path.setAttribute("d", d);
      path.setAttribute("fill", "none");
      path.setAttribute("stroke", weight > 0.2 ? "#a855f7" : "#555");
      path.setAttribute("stroke-width", strokeW);
      path.setAttribute("stroke-opacity", opacity);
      path.setAttribute("stroke-linecap", "round");
      path.style.cssText = "opacity:0;transition:opacity 0.6s ease " + (0.5 + i * 0.08) + "s;";

      /* Arrow tip */
      var circle = document.createElementNS(svgNS, "circle");
      circle.setAttribute("cx", tx);
      circle.setAttribute("cy", ty);
      circle.setAttribute("r", Math.max(2.5, weight * 5));
      circle.setAttribute("fill", weight > 0.2 ? "#a855f7" : "#555");
      circle.setAttribute("fill-opacity", opacity);
      circle.style.cssText = "opacity:0;transition:opacity 0.6s ease " + (0.5 + i * 0.08) + "s;";

      svg.appendChild(path);
      svg.appendChild(circle);
      setTimeout(function() { path.style.opacity = "1"; circle.style.opacity = "1"; }, 100);
    });

    /* Show weight labels */
    tokenEls.forEach(function(t) {
      setTimeout(function() { t.wl.style.opacity = "1"; }, 100);
    });
  }, 600);
}

/* ── Scene 12: Attention Heatmap ── */
function attS2() {
  var box = $("attn-heatmap-container");
  box.textContent = "";
  var T = ATT_TOKENS.length;

  var title = document.createElement("div");
  title.className = "attn-grid-title";
  title.textContent = "jllm \u2014 Layer 0, Head 0";
  box.appendChild(title);

  var sub = document.createElement("div");
  sub.className = "attn-grid-subtitle";
  sub.textContent = "Row = \"who is asking\" \u2022 Column = \"who is answering\" \u2022 Hover to inspect";
  box.appendChild(sub);

  /* Grid: (T+1) columns for row labels + T data cols */
  var grid = document.createElement("div");
  grid.className = "attn-grid";
  grid.style.gridTemplateColumns = "60px repeat(" + T + ", 1fr)";
  grid.style.maxWidth = "700px";
  grid.style.margin = "0 auto";

  /* Column headers */
  var corner = document.createElement("div");
  grid.appendChild(corner);
  ATT_TOKENS.forEach(function(tok) {
    var lbl = document.createElement("div");
    lbl.className = "attn-axis-label";
    lbl.textContent = tok.text;
    lbl.style.fontSize = "9px";
    lbl.style.transform = "rotate(-45deg)";
    lbl.style.height = "50px";
    grid.appendChild(lbl);
  });

  /* Tooltip element */
  var tooltip = document.createElement("div");
  tooltip.className = "attn-tooltip";
  tooltip.style.display = "none";
  box.appendChild(tooltip);

  /* Data rows */
  HEATMAP.forEach(function(row, ri) {
    /* Row label */
    var rlbl = document.createElement("div");
    rlbl.className = "attn-axis-label row-label";
    rlbl.textContent = ATT_TOKENS[ri].text;
    rlbl.style.fontSize = "10px";
    grid.appendChild(rlbl);

    row.forEach(function(val, ci) {
      var cell = document.createElement("div");
      cell.className = "attn-cell";
      if (ci > ri) {
        /* Future: causal mask */
        cell.style.background = "#0a0a0a";
      } else {
        var intensity = Math.pow(val, 0.6);
        var r = Math.round(168 * intensity);
        var g = Math.round(85 * intensity * 0.5);
        var b = Math.round(247 * intensity);
        cell.style.background = "rgb(" + r + "," + g + "," + b + ")";
      }
      cell.style.opacity = "0";
      cell.style.transition = "opacity 0.3s ease " + (ri * 0.05 + ci * 0.02) + "s";

      (function(rr, cc, vv) {
        cell.addEventListener("mouseenter", function(e) {
          tooltip.textContent = ATT_TOKENS[rr].text + " \u2192 " + ATT_TOKENS[cc].text + " = " + vv.toFixed(3);
          tooltip.style.display = "block";
          tooltip.style.left = (e.clientX + 12) + "px";
          tooltip.style.top = (e.clientY - 30) + "px";
        });
        cell.addEventListener("mousemove", function(e) {
          tooltip.style.left = (e.clientX + 12) + "px";
          tooltip.style.top = (e.clientY - 30) + "px";
        });
        cell.addEventListener("mouseleave", function() {
          tooltip.style.display = "none";
        });
      })(ri, ci, val);

      grid.appendChild(cell);
      setTimeout(function() { cell.style.opacity = "1"; }, 100);
    });
  });

  box.appendChild(grid);

  /* Color legend */
  var legend = document.createElement("div");
  legend.style.cssText = "display:flex;align-items:center;justify-content:center;gap:8px;margin-top:16px;font-size:11px;color:var(--text-3);";
  var low = document.createElement("span");
  low.textContent = "0.0";
  var bar = document.createElement("div");
  bar.style.cssText = "width:120px;height:10px;border-radius:5px;background:linear-gradient(90deg,#0a0a0a,#a855f7);";
  var high = document.createElement("span");
  high.textContent = "1.0";
  legend.appendChild(low);
  legend.appendChild(bar);
  legend.appendChild(high);
  box.appendChild(legend);
}

/* ── Scene 13: Multiple Heads ── */
function attS3() {
  var box = $("multihead-container");
  box.textContent = "";

  var grid = document.createElement("div");
  grid.className = "head-grid";

  for (var h = 0; h < 13; h++) {
    (function(headIdx) {
      var card = document.createElement("div");
      card.className = "head-card";
      if (headIdx === 4 || headIdx === 6 || headIdx === 0) card.className += " highlight";
      card.style.transitionDelay = (headIdx * 0.07) + "s";

      var num = document.createElement("div");
      num.className = "head-num";
      num.textContent = "Head " + headIdx;
      card.appendChild(num);

      var role = document.createElement("div");
      role.className = "head-role";
      role.textContent = HEAD_ROLES[headIdx];
      card.appendChild(role);

      /* Top 5 attention bars for this head */
      var weights = MULTIHEAD[headIdx];
      var indexed = weights.map(function(w, i) { return {w: w, i: i}; });
      indexed.sort(function(a, b) { return b.w - a.w; });
      var top5 = indexed.slice(0, 5);

      top5.forEach(function(item) {
        var row = document.createElement("div");
        row.className = "head-bar-row";

        var label = document.createElement("div");
        label.className = "head-bar-label";
        label.textContent = ATT_TOKENS[item.i].text;
        row.appendChild(label);

        var bar = document.createElement("div");
        bar.className = "head-bar";
        bar.style.width = "0%";
        bar.style.background = item.w > 0.3 ? "linear-gradient(90deg,#a855f7,#ec4899)" :
                               item.w > 0.1 ? "#a855f7" : "#333";
        row.appendChild(bar);

        card.appendChild(row);

        setTimeout(function() {
          bar.style.width = (item.w * 100) + "%";
        }, 400 + headIdx * 70);
      });

      grid.appendChild(card);
      setTimeout(function() { card.classList.add("visible"); }, 100);
    })(h);
  }

  box.appendChild(grid);
}

/* ── Scene 14: Evolution Across Layers ── */
function attS4() {
  var box = $("evolution-container");
  box.textContent = "";

  var layerKeys = ["layer_0", "layer_6", "layer_12", "layer_18", "layer_25"];

  layerKeys.forEach(function(key, li) {
    var data = EVOLUTION[key];
    var evoDiv = document.createElement("div");
    evoDiv.className = "evo-layer";
    evoDiv.style.transitionDelay = (li * 0.15) + "s";

    /* Label row */
    var labelRow = document.createElement("div");
    labelRow.className = "evo-layer-label";
    var nameSpan = document.createElement("span");
    nameSpan.className = "evo-layer-name";
    nameSpan.textContent = EVO_LABELS[li].name;
    var descSpan = document.createElement("span");
    descSpan.className = "evo-layer-desc";
    descSpan.textContent = EVO_LABELS[li].desc;
    labelRow.appendChild(nameSpan);
    labelRow.appendChild(descSpan);

    /* Find max weight for annotation */
    var maxW = 0, maxI = 0;
    data.forEach(function(w, i) { if (w > maxW) { maxW = w; maxI = i; } });
    var topSpan = document.createElement("span");
    topSpan.style.cssText = "margin-left:auto;font-family:var(--mono);font-size:11px;color:var(--text-2);";
    topSpan.textContent = "\u2192 " + ATT_TOKENS[maxI].text + " (" + maxW.toFixed(2) + ")";
    labelRow.appendChild(topSpan);
    evoDiv.appendChild(labelRow);

    /* Bar chart */
    var bars = document.createElement("div");
    bars.className = "evo-bars";

    data.forEach(function(w, i) {
      if (i >= 12) return; /* skip process and . for cleaner display */
      var col = document.createElement("div");
      col.className = "evo-bar-col";

      var bar = document.createElement("div");
      bar.className = "evo-bar";
      bar.style.height = "0px";
      var isMax = (i === maxI);
      bar.style.background = isMax ? "linear-gradient(180deg,#a855f7,#ec4899)" :
                             w > 0.1 ? "#a855f7" : "#333";
      col.appendChild(bar);

      var tok = document.createElement("div");
      tok.className = "evo-bar-tok";
      tok.textContent = ATT_TOKENS[i].text;
      col.appendChild(tok);

      bars.appendChild(col);

      setTimeout(function() {
        bar.style.height = Math.max(2, w * 55) + "px";
      }, 300 + li * 150);
    });

    evoDiv.appendChild(bars);
    box.appendChild(evoDiv);

    setTimeout(function() { evoDiv.classList.add("visible"); }, 100);
  });
}

/* ── Scene 15: Full Pipeline ── */
function attS5() {
  var box = $("pipeline-container-2");
  box.textContent = "";
  var pipe = document.createElement("div");
  pipe.className = "pipeline";
  var steps = [
    {name:"Raw Text",detail:"Training data",status:"done",cls:"pipe-done"},
    {name:"Tokenizer",detail:"BPE, 32K vocab",status:"done",cls:"pipe-done"},
    {name:"Embeddings",detail:"32,768 \u00d7 1,664",status:"done",cls:"pipe-done"},
    {name:"Attention",detail:"13 heads \u00d7 26 layers",status:"now",cls:"pipe-current"},
    {name:"MLP",detail:"ReLU\u00b2, 4\u00d7 expand",status:"next",cls:"pipe-next"},
    {name:"Prediction",detail:"Next token",status:"next",cls:"pipe-next"}
  ];
  steps.forEach(function(st, si) {
    if (si > 0) {
      var arrow = document.createElement("div");
      arrow.className = "pipe-arrow";
      arrow.textContent = "\u2192";
      pipe.appendChild(arrow);
    }
    var step = document.createElement("div");
    step.className = "pipe-step " + st.cls;
    step.style.opacity = "0";
    step.style.transition = "opacity 0.5s ease " + (si * 0.2) + "s";
    var name = document.createElement("div");
    name.className = "pipe-name";
    name.textContent = st.name;
    step.appendChild(name);
    var detail = document.createElement("div");
    detail.className = "pipe-detail";
    detail.textContent = st.detail;
    step.appendChild(detail);
    var status = document.createElement("div");
    var scls = st.status === "done" ? "pipe-status-done" : (st.status === "now" ? "pipe-status-now" : "pipe-status-next");
    status.className = "pipe-status " + scls;
    status.textContent = st.status === "done" ? "Complete" : (st.status === "now" ? "Chapter 3" : "Up Next");
    step.appendChild(status);
    pipe.appendChild(step);
    setTimeout(function() { step.style.opacity = "1"; }, 100);
  });
  box.appendChild(pipe);
}

/* ── Init ── */
showScene(0);
</script>
</body>
</html>
